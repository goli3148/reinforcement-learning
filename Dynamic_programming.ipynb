{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a1f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Literal\n",
    "import random\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb7c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4.1\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.current_state = (1, 1)\n",
    "        self.end_states = [(0, 0), (3, 3)]\n",
    "        self.BOARD_SIZE = 4\n",
    "        self.random_environment = 0\n",
    "\n",
    "    def is_final(self, state):\n",
    "        return state in self.end_states\n",
    "\n",
    "    def available_actions(self):\n",
    "        return [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        actions = self.available_actions()\n",
    "\n",
    "        transitions = []\n",
    "        intended_prob = 1\n",
    "        other_prob = 0.0 / (len(actions) - 1)\n",
    "\n",
    "        for a in actions:\n",
    "            if a == action:\n",
    "                prob = intended_prob\n",
    "            else:\n",
    "                prob = other_prob\n",
    "\n",
    "            next_state, reward = self._deterministic_step(state, a)\n",
    "            transitions.append((prob, next_state, reward))\n",
    "\n",
    "        return transitions\n",
    "\n",
    "    def _deterministic_step(self, state, action):\n",
    "        if action == \"up\":\n",
    "            next_state = (state[0] - 1, state[1])\n",
    "        elif action == \"down\":\n",
    "            next_state = (state[0] + 1, state[1])\n",
    "        elif action == \"left\":\n",
    "            next_state = (state[0], state[1] - 1)\n",
    "        elif action == \"right\":\n",
    "            next_state = (state[0], state[1] + 1)\n",
    "\n",
    "        if not self.is_valid_state(next_state):\n",
    "            next_state = state\n",
    "\n",
    "        reward = -1\n",
    "\n",
    "        return next_state, reward\n",
    "\n",
    "    def is_valid_state(self, state):\n",
    "        return 0 <= state[0] < 4 and 0 <= state[1] < 4\n",
    "    \n",
    "class FrozenLake:\n",
    "    def __init__(self):\n",
    "        self.current_state = (0, 0)\n",
    "        self.end_states = [(3, 3)]\n",
    "        self.traps = [(1, 1), (1, 3), (2, 3), (3, 0)]\n",
    "        self.BOARD_SIZE = 4\n",
    "        self.random_environment = 0\n",
    "\n",
    "    def is_final(self, state):\n",
    "        return state in self.end_states\n",
    "    \n",
    "    def is_trap(self, state):\n",
    "        return state in self.traps\n",
    "\n",
    "    def available_actions(self):\n",
    "        return [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        actions = self.available_actions()\n",
    "\n",
    "        transitions = []\n",
    "        intended_prob = 0.7\n",
    "        other_prob = 0.3 / (len(actions) - 1)\n",
    "\n",
    "        for a in actions:\n",
    "            if a == action:\n",
    "                prob = intended_prob\n",
    "            else:\n",
    "                prob = other_prob\n",
    "\n",
    "            next_state, reward = self._deterministic_step(state, a)\n",
    "            transitions.append((prob, next_state, reward))\n",
    "\n",
    "        return transitions\n",
    "\n",
    "    def _deterministic_step(self, state, action):\n",
    "        if action == \"up\":\n",
    "            next_state = (state[0] - 1, state[1])\n",
    "        elif action == \"down\":\n",
    "            next_state = (state[0] + 1, state[1])\n",
    "        elif action == \"left\":\n",
    "            next_state = (state[0], state[1] - 1)\n",
    "        elif action == \"right\":\n",
    "            next_state = (state[0], state[1] + 1)\n",
    "\n",
    "        if not self.is_valid_state(next_state):\n",
    "            next_state = state\n",
    "\n",
    "        if self.is_final(next_state):\n",
    "            reward = 1\n",
    "        elif self.is_trap(next_state):\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        return next_state, reward\n",
    "\n",
    "    def is_valid_state(self, state):\n",
    "        return 0 <= state[0] < 4 and 0 <= state[1] < 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation():\n",
    "    grid = FrozenLake()\n",
    "    discounted_factor = 1\n",
    "    state_value = [[random.random() for _ in range(grid.BOARD_SIZE)] for _ in range(grid.BOARD_SIZE)]\n",
    "    last_state_value = [[random.random() for _ in range(grid.BOARD_SIZE)] for _ in range(grid.BOARD_SIZE)]\n",
    "    actions = grid.available_actions()\n",
    "    eps = 1e-4\n",
    "    action_prob = 1 / 4\n",
    "\n",
    "    def difference_states():\n",
    "        total_diff = 0\n",
    "        for i in range(grid.BOARD_SIZE):\n",
    "            for j in range(grid.BOARD_SIZE):\n",
    "                total_diff += abs(state_value[i][j] - last_state_value[i][j])  # absolute difference\n",
    "        return total_diff\n",
    "\n",
    "    while difference_states() >= eps:\n",
    "        state_value = copy.deepcopy(last_state_value)\n",
    "        for i in range(grid.BOARD_SIZE):\n",
    "            for j in range(grid.BOARD_SIZE):\n",
    "                # Search in all states\n",
    "                value = 0\n",
    "                if grid.is_final((i, j)):\n",
    "                    value = 0\n",
    "                else:\n",
    "                    for action in actions:\n",
    "                        for prob, next_state, reward in grid.get_transitions((i,j), action):\n",
    "                            value += action_prob * prob * (reward + discounted_factor * state_value[next_state[0]][next_state[1]])\n",
    "                last_state_value[i][j] = value\n",
    "\n",
    "    for i in range(grid.BOARD_SIZE):\n",
    "        for j in range(grid.BOARD_SIZE):\n",
    "            print(f\"{state_value[i][j]:.2f}\", end=\" \")\n",
    "        print()\n",
    "        \n",
    "\n",
    "policy_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d97ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration():\n",
    "    grid = FrozenLake()\n",
    "    discounted_factor = 0.9\n",
    "    action_prob = 1 / 4\n",
    "\n",
    "    def initialize_random_action():\n",
    "        action = [[None for _ in range(grid.BOARD_SIZE)] for _ in range(grid.BOARD_SIZE)]\n",
    "        for i in range(grid.BOARD_SIZE):\n",
    "            for j in range(grid.BOARD_SIZE):\n",
    "                if grid.is_final((i, j)):\n",
    "                    continue\n",
    "                action[i][j] = random.choice(grid.available_actions())\n",
    "        return action\n",
    "            \n",
    "\n",
    "    actions = initialize_random_action()\n",
    "    new_actions = initialize_random_action()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # policy evaluation over deterministic policy\n",
    "    def policy_evaluation():\n",
    "        last_state_value = [[random.random() for _ in range(grid.BOARD_SIZE)] for _ in range(grid.BOARD_SIZE)]\n",
    "        state_value = [[random.random() for _ in range(grid.BOARD_SIZE)] for _ in range(grid.BOARD_SIZE)]\n",
    "        eps = 1e-14\n",
    "        def difference_states():\n",
    "            total_diff = 0\n",
    "            for i in range(grid.BOARD_SIZE):\n",
    "                for j in range(grid.BOARD_SIZE):\n",
    "                    total_diff += abs(state_value[i][j] - last_state_value[i][j])  # absolute difference\n",
    "            return total_diff\n",
    "\n",
    "        while difference_states() >= eps:\n",
    "            state_value = copy.deepcopy(last_state_value)\n",
    "            for i in range(grid.BOARD_SIZE):\n",
    "                for j in range(grid.BOARD_SIZE):\n",
    "                    # Search in all states\n",
    "                    value = 0\n",
    "                    if grid.is_final((i, j)):\n",
    "                        value = 0\n",
    "                    else:\n",
    "                        for prob, next_state, reward in grid.get_transitions((i,j), actions[i][j]):\n",
    "                            value += prob * (reward + discounted_factor * state_value[next_state[0]][next_state[1]])\n",
    "                    last_state_value[i][j] = value\n",
    "        return state_value\n",
    "\n",
    "    def policy_improvement(state_value):\n",
    "        for i in range(grid.BOARD_SIZE):\n",
    "            for j in range(grid.BOARD_SIZE):\n",
    "\n",
    "                if grid.is_final((i, j)):\n",
    "                    new_actions[i][j] = None\n",
    "                    continue\n",
    "\n",
    "                max_value = -float('inf')\n",
    "                for action in grid.available_actions():\n",
    "                    value = 0\n",
    "                    for prob, next_state, reward in grid.get_transitions((i,j), action):\n",
    "                        value += prob * (reward + discounted_factor * state_value[next_state[0]][next_state[1]])\n",
    "                    if value > max_value:\n",
    "                        max_value = value\n",
    "                        new_actions[i][j] = action\n",
    "        \n",
    "    \n",
    "    while not actions == new_actions:\n",
    "        actions = copy.deepcopy(new_actions)\n",
    "        state_value = policy_evaluation()\n",
    "        policy_improvement(state_value)\n",
    "        \n",
    "        \n",
    "\n",
    "    for i in range(grid.BOARD_SIZE):\n",
    "        for j in range(grid.BOARD_SIZE):\n",
    "            print(f\"{state_value[i][j]:.2f}\", end=\" \")\n",
    "        print()\n",
    "\n",
    "    for i in range(grid.BOARD_SIZE):\n",
    "        for j in range(grid.BOARD_SIZE):\n",
    "            print(f\"{actions[i][j]}\", end=\" \")\n",
    "        print()\n",
    "    \n",
    "policy_iteration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4a0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration():\n",
    "    grid = FrozenLake()\n",
    "    discounted_factor = .9\n",
    "    state_value = [[random.random() for _ in range(grid.BOARD_SIZE)] for _ in range(grid.BOARD_SIZE)]\n",
    "    last_state_value = [[random.random() for _ in range(grid.BOARD_SIZE)] for _ in range(grid.BOARD_SIZE)]\n",
    "    actions = [[None for _ in range(grid.BOARD_SIZE)] for _ in range(grid.BOARD_SIZE)]\n",
    "    eps = 1e-4\n",
    "\n",
    "    def difference_states():\n",
    "        total_diff = 0\n",
    "        for i in range(grid.BOARD_SIZE):\n",
    "            for j in range(grid.BOARD_SIZE):\n",
    "                total_diff += abs(state_value[i][j] - last_state_value[i][j])  # absolute difference\n",
    "        return total_diff\n",
    "    \n",
    "    while difference_states() >= eps:\n",
    "        last_state_value = copy.deepcopy(state_value)\n",
    "\n",
    "        for i in range(grid.BOARD_SIZE):\n",
    "            for j in range(grid.BOARD_SIZE):\n",
    "\n",
    "                if grid.is_final((i, j)) or grid.is_trap((i, j)):\n",
    "                    state_value[i][j] = 0\n",
    "                    continue\n",
    "\n",
    "                max_value = -float('inf')\n",
    "                for action in grid.available_actions():\n",
    "                    value = 0\n",
    "                    for prob, next_state, reward in grid.get_transitions((i,j), action):\n",
    "                        if next_state == (i,j): reward =  -1\n",
    "                        value += prob * (reward + discounted_factor * state_value[next_state[0]][next_state[1]])\n",
    "                    \n",
    "                    if value > max_value:\n",
    "                        state_value[i][j] = value\n",
    "                        actions[i][j] = action\n",
    "                        max_value = value\n",
    "\n",
    "    \n",
    "    for i in range(grid.BOARD_SIZE):\n",
    "        for j in range(grid.BOARD_SIZE):\n",
    "            print(f\"{state_value[i][j]:.2f}\", end=\" \")\n",
    "        print()\n",
    "\n",
    "    for i in range(grid.BOARD_SIZE):\n",
    "        for j in range(grid.BOARD_SIZE):\n",
    "            print(f\"{actions[i][j]}\", end=\" \")\n",
    "        print()\n",
    "    \n",
    "value_iteration()\n",
    "                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
